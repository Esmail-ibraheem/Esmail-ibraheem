**About Me** AI Research Engineer with a deep passion for neural networks, large language models, and cutting-edge machine  
learning techniques. Skilled in designing and optimizing scalable systems with a strong foundation in PyTorch 
and CUDA. Committed to open-source development, multilingual AI accessibility, and building intuitive tools   
that bridge the gap between research and real-world applications by translating them into practical implementations.      

**For more, check out my:** 
## [Website](https://esmail-ibraheem.github.io/portfolio/)

### **Publications:** _ordered by most proud of_
1. **[ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses ](https://arxiv.org/abs/2504.08744)**
2. **[Galvatron: Automatic Distributed Training for Large Transformer Models](https://arxiv.org/abs/2504.03662)**
3. **[Theoretical Foundations and Mitigation of Hallucination in Large Language Models ](https://arxiv.org/abs/2507.22915)**
4. **[Mixture of Transformers: Macro-Level Gating for Sparse Activation in Large Language Model Ensembles](http://dx.doi.org/10.13140/RG.2.2.25049.02400)**
5. **[Bachelor Thesis: AI Engine: Deep Learning and Neural Network Engine](http://dx.doi.org/10.13140/RG.2.2.22814.24643)**
6. **[Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)**
7. **[Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)**

 
<!--
**Esmail-ibraheem/Esmail-ibraheem** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
